<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Transformer From Scratch | Youness El Brag</title>
    <meta name="author" content="Youness El Brag">
    <meta name="description" content="Implmentation of -Transformer following Series of Article About Vision-Langauge model , in project i implmented from Scratch using numpy">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://youness-elbrag.github.io//projects/10_project/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Youness </span>El Brag</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/courses/">Courses</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Transformer From Scratch</h1>
            <p class="post-description">Implmentation of -Transformer following Series of Article About Vision-Langauge model , in project i implmented from Scratch using numpy</p>
          </header>

          <article>
            <h3 id="transformers-numpy">Transformers Numpy</h3>

<p>this is Implementation of Transfomrers Numpy Version from Scratch which all LLM based on have abetter understaning cam help to build this type of model in right concept 
Welcome to the repository for the TransformersNumpy-Version project! This project focuses on implementing Transformers, a groundbreaking model in the field of natural language processing and machine learning, using the powerful numpy library.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0" align="center">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/project10/full-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/project10/full-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/project10/full-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/project10/full.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>

<h3 id="introduction">Introduction</h3>

<p>Transformers have revolutionized the way we process and understand natural language, enabling breakthroughs in tasks such as machine translation, sentiment analysis, and question-answering systems. This repository aims to provide a comprehensive implementation of Transformers using numpy, showcasing the core concepts and functionalities of this powerful model.</p>

<h3 id="key-features">Key Features</h3>

<ul>
  <li>
<strong>Numpy Implementation:</strong> The implementation in this repository heavily relies on the numpy library, allowing for efficient computations and easy-to-understand code.</li>
  <li>
<strong>Full Implementation:</strong> The repository provides a complete implementation of Transformers, including attention mechanisms, positional encoding, and feed-forward networks.
.</li>
</ul>

<h3 id="contents">Contents</h3>

<p>Here’s an overview of the contents you’ll find in this repository:</p>

<ul>
  <li>
<strong><code class="language-plaintext highlighter-rouge">Decoder.py and Encoder.py</code>:</strong> This file contains the main implementation of the Transformer model using numpy. It includes classes and functions for attention mechanisms, positional encoding, and the overall Transformer architecture.</li>
  <li>
<strong><code class="language-plaintext highlighter-rouge">LayersNumpy.py</code>:</strong> This file provides utility functions for data preprocessing and handling, enabling seamless integration with different datasets.</li>
  <li>
<strong><code class="language-plaintext highlighter-rouge">transfomers-explained-mathematic-and-code-in-depth.ipynb</code>:</strong> This Jupyter Notebook serves as an example to showcase how to use the Transformer model implemented in this repository. It includes a step-by-step walkthrough of training and evaluating the model on a specific task.</li>
  <li>
<strong><code class="language-plaintext highlighter-rouge">main.py</code>:</strong> this provide full combinations of the blocks Transformser.</li>
</ul>

<h3 id="self-attention-math">Self-Attention Math</h3>

<div class="row">
    <div class="col-sm mt-3 mt-md-0" align="center">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/project10/att-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/project10/att-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/project10/att-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/project10/att.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">Self_Attention</span><span class="p">(</span><span class="n">input_embedding</span> <span class="p">,</span><span class="n">WieghtMatrix_QKY</span><span class="p">,</span> <span class="n">out_wieghts</span><span class="p">,</span><span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Self-Attention take input of emebeding matrix which asseccoite with 
    the Positional encoding ww will cover later in section 
    Query and Key and Value all of them have the same dimession as the input 
    </span><span class="sh">"""</span>
    <span class="k">try</span> <span class="p">:</span> 
        <span class="k">if</span> <span class="n">batch_first</span><span class="o">==</span><span class="bp">True</span><span class="p">:</span>
            <span class="n">Query</span> <span class="p">,</span> <span class="n">Key</span> <span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">input_embedding</span><span class="nd">@WieghtMatrix_QKY</span> <span class="p">,</span> <span class="mi">3</span> <span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">mask</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">input_embedding</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>\
                    <span class="sa">f</span><span class="sh">"</span><span class="s">input dimession of mask doesn</span><span class="sh">'</span><span class="s">t match with dimession of embedding input:</span><span class="si">{</span><span class="n">mask</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">input_embedding</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span>
                <span class="n">Attention</span> <span class="o">=</span> <span class="nc">Softmax</span><span class="p">(</span><span class="n">Key</span><span class="nd">@Query.swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">input_embedding</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="n">mask</span><span class="p">)</span> 
                <span class="k">return</span>  <span class="n">Attention</span><span class="nd">@value@out_wieghts</span> <span class="p">,</span> <span class="n">Attention</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">Attention</span> <span class="o">=</span> <span class="nc">Softmax</span><span class="p">(</span><span class="n">Key</span><span class="nd">@Query.swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">input_embedding</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span> 
                <span class="k">return</span>  <span class="n">Attention</span><span class="nd">@value@out_wieghts</span> <span class="p">,</span> <span class="n">Attention</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">Exception</span><span class="p">(</span><span class="sh">"</span><span class="s">Batch argumment is missing</span><span class="sh">"</span><span class="p">)</span>

</code></pre></div></div>

<h3 id="12-multi-head-attention">1.2 Multi-Head Attention</h3>

<div class="row">
    <div class="col-sm mt-3 mt-md-0" align="center">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/project10/heads.gif-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/project10/heads.gif-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/project10/heads.gif-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/project10/heads.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">multiHeads_Attention</span><span class="p">(</span><span class="n">input_embedding</span> <span class="p">,</span><span class="n">wieghtsMatrix_QKY</span> <span class="p">,</span> <span class="n">heads</span> <span class="p">,</span><span class="n">out_Wieght</span> <span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">B</span> <span class="p">,</span> <span class="n">seq_len</span> <span class="p">,</span> <span class="n">embed_size</span> <span class="o">=</span> <span class="n">input_embedding</span><span class="p">.</span><span class="n">shape</span>
    <span class="c1"># we have dim input of B . seq_len , 
</span>    <span class="c1"># embed_size ==&gt; B, seq_len , embed_size/ heads 
</span>    <span class="c1">#=&gt; Swape axis into [batch , heads , seq_len , embe_size / heads ]
</span>    <span class="n">Query</span> <span class="p">,</span> <span class="n">Key</span><span class="p">,</span> <span class="n">Value</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">input_embedding</span><span class="nd">@wieghtsMatrix_QKY</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">Query</span> <span class="p">,</span> <span class="n">Key</span><span class="p">,</span> <span class="n">Value</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span> <span class="p">,</span> <span class="n">seq_len</span> <span class="p">,</span><span class="n">heads</span> <span class="p">,</span> <span class="p">(</span><span class="n">embed_size</span> <span class="o">//</span> <span class="n">heads</span><span class="p">)).</span><span class="nf">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="p">(</span><span class="n">Query</span> <span class="p">,</span> <span class="n">Key</span><span class="p">,</span> <span class="n">Value</span><span class="p">)]</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">atten</span> <span class="o">=</span> <span class="nc">Softmax</span><span class="p">(</span><span class="n">Key</span><span class="nd">@Query.swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">embed_size</span> <span class="o">//</span> <span class="n">heads</span><span class="p">)</span> <span class="o">+</span> <span class="n">mask</span> <span class="p">)</span> 
        <span class="nf">return </span><span class="p">(</span><span class="n">atten</span><span class="nd">@Value</span><span class="p">).</span><span class="nf">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span> <span class="p">,</span> <span class="n">seq_len</span> <span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span><span class="nd">@out_Wieght</span> <span class="p">,</span> <span class="n">atten</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">atten</span> <span class="o">=</span> <span class="nc">Softmax</span><span class="p">(</span><span class="n">Key</span><span class="nd">@Query.swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">embed_size</span> <span class="o">//</span> <span class="n">heads</span><span class="p">))</span> 
        <span class="nf">return </span><span class="p">(</span><span class="n">atten</span><span class="nd">@Value</span><span class="p">).</span><span class="nf">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span> <span class="p">,</span> <span class="n">seq_len</span> <span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span><span class="nd">@out_Wieght</span> <span class="p">,</span> <span class="n">atten</span>
</code></pre></div></div>

<h3 id="13-scale-dot-product-attention">1.3 Scale Dot Product Attention</h3>

<div class="row">
    <div class="col-sm mt-3 mt-md-0" align="center">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/project10/mha_img_original-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/project10/mha_img_original-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/project10/mha_img_original-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/project10/mha_img_original.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ScaleDotProductAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    compute scale dot product attention

    Query : given sentence that we focused on (decoder)
    Key : every sentence to check relationship with Qeury(encoder)
    Value : every sentence same with Key (encoder)
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">config</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ScaleDotProductAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="sh">"</span><span class="s">attention_droput</span><span class="sh">"</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span><span class="n">output_attentions</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="c1"># input is 4 dimension tensor
</span>        <span class="c1"># [batch_size, head, length, d_tensor]
</span>        <span class="n">batch_size</span><span class="p">,</span> <span class="n">head</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">d_tensor</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>

        <span class="c1"># 1. dot product Query with Key^T to compute similarity
</span>        <span class="n">k_t</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># transpose
</span>        <span class="n">score</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k_t</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">d_tensor</span><span class="p">)</span>  <span class="c1"># scaled dot product
</span>
        <span class="c1"># 3. pass them softmax to make [0, 1] range
</span>        <span class="n">score</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span>  <span class="n">self</span><span class="p">.</span><span class="nf">attention_dropout</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

        <span class="c1"># 4. multiply with Value
</span>        <span class="n">v</span> <span class="o">=</span> <span class="n">score</span> <span class="o">@</span> <span class="n">v</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="nf">return </span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>

        <span class="nf">return </span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>
</code></pre></div></div>
<h3 id="positional-encodeing">Positional Encodeing</h3>

<div class="row">
    <div class="col-sm mt-3 mt-md-0" align="center">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/project10/pos-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/project10/pos-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/project10/pos-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/project10/pos.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span> 
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span> 

<span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    compute sinusoid encoding.
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">max_len</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span><span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>    
        <span class="sh">"""</span><span class="s">
        constructor of sinusoid encoding class

        :param d_model: dimension of model
        :param max_len: max sequence length
        :param device: hardware device setting
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1"># same size with input matrix (for adding with input matrix)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">encoding</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoding</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>  <span class="c1"># we don't need to compute gradient
</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">pos</span><span class="p">.</span><span class="nf">float</span><span class="p">().</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># 1D =&gt; 2D unsqueeze to represent word's position
</span>
        <span class="n">_2i</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">2</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
        <span class="c1"># 'i' means index of d_model (e.g. embedding size = 50, 'i' = [0,50])
</span>        <span class="c1"># "step=2" means 'i' multiplied with two (same with 2 * i)
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoding</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10000</span> <span class="o">**</span> <span class="p">(</span><span class="n">_2i</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoding</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10000</span> <span class="o">**</span> <span class="p">(</span><span class="n">_2i</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)))</span>
        <span class="c1"># compute positional encoding to consider positional information of words
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># self.encoding
</span>        <span class="c1"># [max_len = 512, d_model = 512]
</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>
        <span class="c1"># [batch_size = 128, seq_len = 30]
</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">encoding</span><span class="p">[:</span><span class="n">seq_len</span><span class="p">,</span> <span class="p">:]</span>
        <span class="c1"># [seq_len = 30, d_model = 512]
</span>        <span class="c1"># it will add with tok_emb : [128, 30, 512]  
</span>
</code></pre></div></div>

<h3 id="residual-cennection">Residual Cennection</h3>

<div class="row">
    <div class="col-sm mt-3 mt-md-0" align="center">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/project10/res-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/project10/res-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/project10/res-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/project10/res.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">ResidualCennection</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span> <span class="n">residual</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ResidualCennection</span><span class="p">,</span><span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pass_trough</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">self</span><span class="p">.</span><span class="n">addtion</span> <span class="o">=</span> <span class="n">residual</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">pass_trough</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">addtion</span>
</code></pre></div></div>

<h3 id="14-layer-norm-and-softmax">1.4 Layer Norm And Softmax</h3>

<div class="row">
    <div class="col-sm mt-3 mt-md-0" align="center">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/project10/layer_norm-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/project10/layer_norm-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/project10/layer_norm-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/project10/layer_norm.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

<span class="k">def</span> <span class="nf">NormLayar</span><span class="p">(</span><span class="n">Z</span><span class="p">):</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">Z</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span> <span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">Z</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span> <span class="p">,</span> <span class="n">keepdims</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
    <span class="nf">return </span><span class="p">((</span><span class="n">Z</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">var</span><span class="p">))</span> <span class="o">+</span> <span class="n">eps</span>

<span class="k">def</span> <span class="nf">ReLU</span><span class="p">(</span><span class="n">Z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
    

<span class="k">def</span> <span class="nf">Softmax</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    descriptions function : Softmax is non-linear function that give the averege of between 
    0 and 1 of in element in matrix 
    </span><span class="sh">"""</span>
    <span class="n">e_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="n">z</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span><span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">e_x</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">e_x</span> <span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span> <span class="p">,</span><span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

</code></pre></div></div>

<h3 id="15-mlp">1.5 MLP</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    A multi-layer perceptron module.
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dense_1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="sh">"</span><span class="s">embedding_size</span><span class="sh">"</span><span class="p">],</span><span class="n">config</span><span class="p">[</span><span class="sh">"</span><span class="s">mlp_ratio</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="n">config</span><span class="p">[</span><span class="sh">"</span><span class="s">embedding_size</span><span class="sh">"</span><span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dense_2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="sh">"</span><span class="s">embedding_size</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="n">config</span><span class="p">[</span><span class="sh">"</span><span class="s">mlp_ratio</span><span class="sh">"</span><span class="p">],</span> <span class="n">config</span><span class="p">[</span><span class="sh">"</span><span class="s">embedding_size</span><span class="sh">"</span><span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="sh">"</span><span class="s">hidden_dropout_prob</span><span class="sh">"</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dense_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dense_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

</code></pre></div></div>

<h3 id="16-transformerencoder">1.6 TransformerEncoder</h3>

<div class="row">
    <div class="col-sm mt-3 mt-md-0" align="center">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/project10/enc-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/project10/enc-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/project10/enc-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/project10/enc.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span> 
<span class="kn">from</span> <span class="n">layer</span> <span class="kn">import</span> <span class="o">*</span> 
<span class="kn">from</span> <span class="n">layerNumpy</span> <span class="kn">import</span> <span class="o">*</span>

<span class="k">def</span> <span class="nf">TransfomerEncoder</span><span class="p">(</span><span class="n">embed_input</span><span class="p">,</span><span class="n">mask</span> <span class="p">,</span>
                      <span class="n">head</span><span class="p">,</span> <span class="n">Wieghts_QKY</span> <span class="p">,</span> 
                      <span class="n">Wieghts_out</span> <span class="p">,</span><span class="n">FullyLinear1</span><span class="p">,</span> 
                      <span class="n">FullLinear2</span> <span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
    <span class="n">input_embedding</span> <span class="o">=</span> <span class="n">embed_input</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>
    <span class="n">multiHeads</span> <span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">multiHeads_Attention</span><span class="p">(</span><span class="n">input_embedding</span><span class="p">,</span>
                                     <span class="n">Wieghts_QKY</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="n">T</span><span class="p">,</span>
                                     <span class="n">head</span><span class="p">,</span>
                                     <span class="n">Wieghts_out</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="n">T</span><span class="p">,</span>  
                                     <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    <span class="n">Residual</span> <span class="o">=</span> <span class="nc">NormLayar</span><span class="p">((</span><span class="n">input_embedding</span> <span class="o">+</span> <span class="n">multiHeads</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span> <span class="p">)</span>
    
    <span class="n">output</span> <span class="o">=</span> <span class="nc">NormLayar</span><span class="p">((</span><span class="n">Residual</span> <span class="o">+</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">Residual</span><span class="p">,</span><span class="n">FullyLinear1</span><span class="p">))</span><span class="nd">@FullLinear2</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>
<h3 id="16-transformerdecoder">1.6 TransformerDecoder</h3>

<div class="row">
    <div class="col-sm mt-3 mt-md-0" align="center">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/project10/dec-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/project10/dec-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/project10/dec-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/project10/dec.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span> 
<span class="kn">from</span> <span class="n">layer</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="n">layerNumpy</span> <span class="kn">import</span> <span class="o">*</span>


<span class="k">def</span> <span class="nf">TransfomerDecoder</span><span class="p">(</span><span class="n">embed_input</span><span class="p">,</span><span class="n">mask</span> <span class="p">,</span>
                      <span class="n">head</span><span class="p">,</span> <span class="n">Wieghts_QKY</span> <span class="p">,</span> 
                      <span class="n">Wieghts_out</span> <span class="p">,</span><span class="n">FullyLinear1</span><span class="p">,</span> 
                      <span class="n">FullLinear2</span> <span class="p">,</span> <span class="n">eps</span><span class="p">,</span><span class="n">enc</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">input_embedding</span> <span class="o">=</span> <span class="n">embed_input</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>
    <span class="n">multiHeads</span> <span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">multiHeads_Attention</span><span class="p">(</span><span class="n">input_embedding</span><span class="p">,</span>
                                     <span class="n">Wieghts_QKY</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="n">T</span><span class="p">,</span>
                                     <span class="n">head</span><span class="p">,</span>
                                     <span class="n">Wieghts_out</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="n">T</span><span class="p">,</span>  
                                     <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    <span class="n">Residual</span> <span class="o">=</span> <span class="nc">NormLayar</span><span class="p">((</span><span class="n">input_embedding</span> <span class="o">+</span> <span class="n">multiHeads</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span> <span class="p">)</span>
    
    <span class="k">if</span> <span class="n">enc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">Query</span> <span class="p">,</span> <span class="n">key</span>  <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">enc</span> <span class="p">,</span> <span class="mi">2</span> <span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">enc_</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">Query</span><span class="p">[:,:,:</span><span class="mi">16</span><span class="p">]</span> <span class="p">,</span> <span class="n">key</span><span class="p">[:,:,:</span><span class="mi">16</span><span class="p">]</span>  <span class="p">,</span><span class="n">Residual</span><span class="p">[:,:,:</span><span class="mi">32</span><span class="p">]</span> <span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">MaskedMUltiHeads</span> <span class="p">,</span><span class="n">_</span><span class="o">=</span> <span class="nf">multiHeads_Attention</span><span class="p">(</span><span class="n">enc_</span><span class="p">,</span>
                                         <span class="n">Wieghts_QKY</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="n">T</span><span class="p">,</span>
                                         <span class="n">head</span><span class="p">,</span>
                                         <span class="n">Wieghts_out</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="n">T</span><span class="p">,</span>  
                                         <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>
        <span class="n">Residual</span> <span class="o">=</span> <span class="nc">NormLayar</span><span class="p">((</span><span class="n">enc</span> <span class="o">+</span> <span class="n">MaskedMUltiHeads</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span> <span class="p">)</span>
    
    <span class="n">output</span> <span class="o">=</span> <span class="nc">NormLayar</span><span class="p">((</span><span class="n">Residual</span> <span class="o">+</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">Residual</span><span class="p">,</span><span class="n">FullyLinear1</span><span class="p">))</span><span class="nd">@FullLinear2</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>

</code></pre></div></div>

<h3 id="transfomer-model">Transfomer model</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">from</span> <span class="n">blocks.encoder</span> <span class="kn">import</span> <span class="o">*</span> 
<span class="kn">from</span> <span class="n">blocks.decoder</span> <span class="kn">import</span> <span class="o">*</span> 
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">class</span> <span class="nc">Transformer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embedding_size</span> <span class="p">,</span> <span class="n">heads</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span><span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="sh">"""</span><span class="s">
        Blocks models
        -------------
        encoder : is used without the Mask at this stage 
        Decoder : include the Mask and with trick to split the Qurey and Key 
        
        Parametes :
        Wieghst_QKY_Encoder : is Leanrble wieght matrix feed into Encoder
        Wieghst_QKY_Decoder : is Leanrble wieght matrix feed into ncoder
        embed_size : is mebeding_size dimession of input 
        Seq_len : max lenght of vacolublaries 
        Linear_1 : Linear Denes layar of Deooder
        Linear_2 : Linear Denes layar of Deooder after include the ooutput from Encoder
        Linear_encoder : Linear Denes layar of Enooder
        </span><span class="sh">"""</span>
    
        <span class="n">self</span><span class="p">.</span><span class="n">embed_input</span> <span class="o">=</span> <span class="n">embedding_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">heads</span> 
        <span class="n">self</span><span class="p">.</span><span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-12</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_QKY_Encoder</span> <span class="o">=</span> <span class="n">transEncoder</span><span class="p">.</span><span class="n">self_attn</span><span class="p">.</span><span class="n">in_proj_weight</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_Out_Encoder</span> <span class="o">=</span> <span class="n">transEncoder</span><span class="p">.</span><span class="n">self_attn</span><span class="p">.</span><span class="n">out_proj</span><span class="p">.</span><span class="n">weight</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_QKY_Decoder</span> <span class="o">=</span> <span class="n">transDecoder</span><span class="p">.</span><span class="n">self_attn</span><span class="p">.</span><span class="n">in_proj_weight</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_Out_Decoder</span> <span class="o">=</span> <span class="n">transDecoder</span><span class="p">.</span><span class="n">self_attn</span><span class="p">.</span><span class="n">out_proj</span><span class="p">.</span><span class="n">weight</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Linear1_Encoder</span> <span class="o">=</span> <span class="n">transEncoder</span><span class="p">.</span><span class="n">linear1</span><span class="p">.</span><span class="n">weight</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Linear2_Encoder</span> <span class="o">=</span> <span class="n">transEncoder</span><span class="p">.</span><span class="n">linear2</span><span class="p">.</span><span class="n">weight</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Linear1_Decoder</span> <span class="o">=</span> <span class="n">transDecoder</span><span class="p">.</span><span class="n">linear1</span><span class="p">.</span><span class="n">weight</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Linear2_Decoder</span> <span class="o">=</span> <span class="n">transDecoder</span><span class="p">.</span><span class="n">linear2</span><span class="p">.</span><span class="n">weight</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">enc_</span> <span class="p">,</span> <span class="n">dec_</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="n">Encoder_</span><span class="o">=</span> <span class="nc">TransfomerEncoder</span><span class="p">(</span><span class="n">enc_</span> <span class="p">,</span><span class="bp">None</span> <span class="p">,</span>
                  <span class="n">self</span><span class="p">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W_QKY_Encoder</span> <span class="p">,</span> 
                  <span class="n">self</span><span class="p">.</span><span class="n">W_Out_Encoder</span> <span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">Linear1_Encoder</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="n">T</span><span class="p">,</span> 
                  <span class="n">self</span><span class="p">.</span><span class="n">Linear2_Encoder</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="n">T</span> <span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">eps</span><span class="p">)</span>

        <span class="n">Decoder_</span> <span class="o">=</span> <span class="nc">TransfomerDecoder</span><span class="p">(</span><span class="n">dec_</span><span class="p">,</span> <span class="n">mask</span> <span class="p">,</span>
                  <span class="n">self</span><span class="p">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W_QKY_Decoder</span> <span class="p">,</span> 
                  <span class="n">self</span><span class="p">.</span><span class="n">W_Out_Decoder</span> <span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">Linear1_Decoder</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="n">T</span><span class="p">,</span> 
                  <span class="n">self</span><span class="p">.</span><span class="n">Linear2_Decoder</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="n">T</span> <span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">eps</span><span class="p">,</span><span class="n">enc</span><span class="o">=</span><span class="n">Encoder_</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">Decoder_</span>


</code></pre></div></div>

          </article>

        </div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Youness El Brag. Last updated: August 01, 2023.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

  </body>
</html>
