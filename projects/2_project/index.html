<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>A noval Mechanisim Attention Filter | Youness El Brag</title>
    <meta name="author" content="Youness El Brag">
    <meta name="description" content="Welcome to the Attention Filter Gate repository! Here, we provide an implementation of our proposed method, the Attention Filter, which is based on the Fast Fourier Transform">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://youness-elbrag.github.io//projects/2_project/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Youness </span>El Brag</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/courses/">Courses</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">A noval Mechanisim Attention Filter</h1>
            <p class="post-description">Welcome to the Attention Filter Gate repository! Here, we provide an implementation of our proposed method, the Attention Filter, which is based on the Fast Fourier Transform</p>
          </header>

          <article>
            <h1 id="attention-filter-gate">Attention Filter Gate</h1>

<p><strong><em>Welcome to the Attention Filter Gate repository! Here, we provide an implementation of our proposed method, the Attention Filter, which is based on the Fast Fourier Transform. In the accompanying PDF document, we explain in detail the steps we have taken to tackle the problem at hand.</em></strong></p>

<h2 id="dataset-description">Dataset Description</h2>

<p>The data used in this project is hosted by a competition on the Kaggle platform, namely the Left Atrial Segmentation Challenge. We have used gmedical images of the left atrium, which are in 3D and come with 30 corresponding masks.</p>

<ul>
  <li>
    <p>Data Source :&lt;/br&gt;</p>

    <p>The Data we have been used is Host in Kaggle Platform competiotion , to download Run following command :
  there’re few step needed to be consider before running the Script</p>

    <ol>
      <li>
        <p><strong>First</strong>:</p>

        <ul>
          <li>Create an account in Kaggle and Get API Token</li>
        </ul>
      </li>
      <li>
        <p><strong>Second</strong>:</p>

        <ul>
          <li>Replace your Kaggle API Tokon which stored in <strong>kaggle.json</strong> in the right path in Script to have authrozation and following this command :
            <div class="language-sh highlighter-rouge">
<div class="highlight"><pre class="highlight"><code> <span class="nb">chmod </span>a+x download.sh <span class="o">&amp;&amp;</span> ./download.sh

</code></pre></div>            </div>
          </li>
        </ul>
      </li>
      <li>
        <p>the Virtualization Sample:</p>

        <p>in here navigate to Dataset folder and open <strong>jupyter notebook</strong> has full virtualization of Medical Images</p>
      </li>
    </ol>
  </li>
</ul>

<div align="center">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/project4/image_label_overlay_animation.gif-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/project4/image_label_overlay_animation.gif-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/project4/image_label_overlay_animation.gif-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/project4/image_label_overlay_animation.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

</div>

<h2 id="introduction">Introduction</h2>

<p>In this project, we aim to build a new mechanism, the Attention Filter Gate, which will address the weaknesses of previous approaches used to handle certain problems, such as:</p>
<ul>
  <li>
<strong>Critical Problems we assign</strong>
    <ul>
      <li>Losing features during extraction when using deep segmentation models</li>
      <li>Handling data with higher resolutions</li>
      <li>Reducing time complexity of training</li>
      <li>Reducing energy consumption during training</li>
      <li>Learning from spatial domain</li>
    </ul>
  </li>
</ul>

<p>Through our exploration of these weaknesses, we aim to provide a better solution to these problems using our proposed Attention Filter Gate mechanism.</p>

<h2 id="main-abstarct-thesis-">Main Abstarct Thesis :</h2>
<ul>
  <li>
<strong>Abstract</strong>
    <ul>
      <li>
        <p>Background:</p>

        <p>Medical imaging diagnosis can be challenging due to low-resolution images caused
  by machine artifacts and patient movement. Researchers have explored algorithms
  and mathematical insights to enhance image quality and representation. One com-
  mon task is segmentation, which requires the detection or localization of diseases
  around the tissue. New approaches and models using artificial intelligence, specif-
  ically computer vision, have been developed to improve the traditional methods
  that have not been entirely effective. Since the publication of the U-Net model
  paper, researchers have focused on building new model architectures to segment
  medical images more effectively. The Transformers model, a core technology be-
  hind many AI applications today, has been a game-changer. The Attention Gate
  was introduced in a paper and used with the U-Net model to increase performance.
  However, it did not solve certain computational cost issues and led researchers to
  investigate how to improve the Attention Gate in a different way while maintaining
  the same structure.</p>
      </li>
      <li>
        <p>Aim :</p>

        <p>The aim was to improve the existing Attention Gate used in U-Net for medical
  image segmentation. The goal was to reduce the computational cost of training
  the model, improve feature extraction, and handle the problem of matrix multi-
  plication used in CNN for feature extraction
  Method
  The Attention Filter Gate was developed to improve upon the Attention Gate.
  Instead of learning from the spatial domain, the model was converted to the fre-
  quency domain using the Fast Fourier Transformation (FFT). A weighted learnable
  matrix was used to filter features in the frequency domain, and FFT was imple-
  mented between up-sampling and down-sampling to reduce matrix multiplication.
  The method tackled computational cost, complexity algorithm, throughput, la-
  tency, FLOP, and enhanced feature extraction.</p>
      </li>
      <li>
        <p>Results</p>

        <p>This study evaluated the performance of two deep learning models, namely Unet and Attention Unet, for image segmentation tasks. The goal was to compare their segmentation accuracy using the mean Dice and mean IoU scores as performance metrics.</p>

        <p>The Unet model, based on the U-Net architecture, achieved a mean Dice score of 0.86 and a mean IoU score of 0.82. These results indicate a good level of segmentation performance, with a substantial overlap and similarity between the predicted segmentations and the ground truth masks.</p>

        <p>On the other hand, the Attention Unet model, which incorporates attention mechanisms into the U-Net architecture, outperformed the standard Unet model. It achieved a mean Dice score of 0.90 and a mean IoU score of 0.86. The higher scores obtained by the Attention Unet model suggest that it effectively captures intricate details and improves the accuracy of the segmentation predictions.</p>

        <p>Overall, the results demonstrate that both the Unet and Attention Unet models are effective for image segmentation tasks. However, the Attention Unet model offers superior performance, surpassing the standard Unet model in terms of both the mean Dice and mean IoU scores. These findings highlight the potential of attention mechanisms in enhancing the accuracy and quality of segmentation results.</p>

        <table>
          <thead>
            <tr>
              <th>Model</th>
              <th>Mean Dice</th>
              <th>Mean IoU</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Unet</td>
              <td>0.86</td>
              <td>0.82</td>
            </tr>
            <tr>
              <td>Attention Unet</td>
              <td>0.90</td>
              <td>0.86</td>
            </tr>
            <tr>
              <td>Attention Filter Unet</td>
              <td>–</td>
              <td>–</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>
        <p>Conclusion:</p>

        <p>This thesis investigates the Attention Filter Gate to address problems such as
  computational cost and feature extraction, providing an alternative approach to
  medical image segmentation that is both efficient and effective. The method en-
  hances feature extraction to reduce information loss between the encoder and
  decoder, and it provides a potential solution for throughput, latency, FLOP, and
  algorithm complexity issues. The Attention Filter Gate improves on the existing
  Attention Gate with intuitive tricks not addressed by previous methods.</p>
      </li>
      <li>
        <p>Keywords:</p>

        <p>Medical Segmentation, Neural networks, Transformers, U-Net model, Attention
  Gate , Fast Fourier Transformation (FFT)</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="setup-the-enviremenet">Setup the Enviremenet</h2>

<p>so Far after Describing the Problem statment now we will look forward to Config our ENV to run the code following Setps :</p>

<ul>
  <li>
<strong>ENV</strong>:
    <ul>
      <li>in this step you will need to create your own env using python following command:
        <div class="language-sh highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>  python <span class="nt">-m</span> venv venv <span class="o">&amp;&amp;</span> <span class="nb">source </span>venv/bin/activate
</code></pre></div>        </div>
      </li>
      <li>you will need to install dependencies of Project has been used following command
        <div class="language-sh highlighter-rouge">
<div class="highlight"><pre class="highlight"><code> pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div>        </div>
        <h2 id="processing-the-data-">Processing the Data :</h2>
      </li>
    </ul>
  </li>
</ul>

<p>after downloading the data by following the guides we provide above , we will need to set the Path of <strong>Images</strong> and <strong>Masks</strong>
in Directory folder Data that contain following these path :</p>

<ul>
  <li>root_img : heart-mri-image-dataset-left-atrial-segmentation/imagesTr</li>
  <li>root_lab : heart-mri-image-dataset-left-atrial-segmentation/labelsTr</li>
</ul>

<p>after getting the right Path run the Script <strong>Post_processing.py</strong> following command</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python Post_processing.py <span class="nt">--root_img</span> heart-mri-image-dataset-left-atrial-segmentation/imagesTr <span class="se">\</span>
<span class="nt">--root_lab</span>  heart-mri-image-dataset-left-atrial-segmentation/labelsTr
</code></pre></div></div>
<p>after the script done you will have a new folder Directory contain the processed image Called <strong>Processed</strong></p>

<h2 id="usage">Usage</h2>

<p>The main script in this project is train.py. It provides command-line arguments for configuring the training process. To run the script check file <strong>config.py</strong> to tune the model based base lines , use the following command to Run Training model using Multi-GPU or single :</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">python</span> <span class="n">train</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">Config</span> <span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">config</span><span class="p">.</span><span class="n">yaml</span> <span class="o">--</span><span class="n">Epochs</span> <span class="mi">100</span> <span class="o">--</span><span class="n">batch_size</span> <span class="mi">16</span> <span class="o">--</span><span class="n">output</span> <span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">output</span>
</code></pre></div></div>
<h4 id="command-line-arguments">Command-line Arguments</h4>

<ol>
  <li>
    <p>The script accepts the following command-line arguments:</p>

    <ul>
      <li>–Config: Path to the configuration file in YAML format.</li>
      <li>–Epochs: Number of training epochs.</li>
      <li>–batch_size: Batch size for training and validation.</li>
      <li>–output: Path to the output directory.</li>
    </ul>
  </li>
  <li>
    <p>Configuration</p>

    <p>The configuration file specified by the –Config argument should be in YAML format. It should contain the following parameters:</p>

    <p>num_workers: Number of workers for data loading.
 train_path: Path to the training data.
 val_path: Path to the validation data.</p>

    <p>Make sure to update the configuration file with the appropriate values for your dataset.
 Training</p>
  </li>
</ol>

<h3 id="training">Training</h3>

<p>1.During the training process, the script performs the following steps:</p>

<ul>
  <li>Initializes the configuration settings.</li>
  <li>Writes the configuration to a YAML file.</li>
  <li>Parses the command-line arguments.</li>
  <li>Reads the configuration from the YAML file.</li>
  <li>Loads the training and validation datasets.</li>
  <li>Initializes the data loaders.</li>
  <li>Defines the loss function.</li>
  <li>Instantiates the segmentation model.</li>
  <li>Sets up callbacks for model checkpointing, CPU usage - - monitoring, and throughput logging.</li>
  <li>Starts the training using the Trainer.fit method.</li>
</ul>

<h2 id="running-prediction">Running Prediction</h2>

<p>To run the prediction script and generate segmentations for your test samples, follow the steps below:</p>

<ol>
  <li>
    <p>Ensure that you have the necessary dependencies installed. If you haven’t installed them yet, refer to the <a href="#Setup">Getting Started</a> section.</p>
  </li>
  <li>
    <p>Open the terminal and navigate to the project directory.</p>
  </li>
  <li>
    <p>Run the following command to execute the <code class="language-plaintext highlighter-rouge">predict.py</code> script:</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="n">python</span> <span class="n">predict</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">Config</span> <span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">config</span><span class="p">.</span><span class="n">yaml</span> <span class="o">--</span><span class="n">sample</span> <span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">test_sample</span><span class="p">.</span><span class="n">nii</span> <span class="o">--</span><span class="n">output</span> <span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">output</span>
</code></pre></div>    </div>
  </li>
</ol>
<div align="center">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/project4/image_label_overlay_over_slice_Prediction_Test.gif-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/project4/image_label_overlay_over_slice_Prediction_Test.gif-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/project4/image_label_overlay_over_slice_Prediction_Test.gif-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/project4/image_label_overlay_over_slice_Prediction_Test.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>


<figcaption>image label overlay over slice Prediction Testt</figcaption>
</div>

<ol>
  <li>
    <p>Note</p>

    <ul>
      <li>
        <p>The –Config argument should point to the YAML configuration file that contains the necessary settings for the prediction.</p>
      </li>
      <li>
        <p>The –sample argument should provide the path to a single test sample in NIfTI format (e.g., .nii or .nii.gz).</p>
      </li>
      <li>
        <p>The –output argument specifies the directory where the generated segmentations will be saved.</p>
      </li>
    </ul>

    <blockquote>
      <p>Make sure that the configuration file, test sample, and output directory paths are correct and accessible.
     &gt;
     &gt; The script will use the latest checkpoint file found in the <code class="language-plaintext highlighter-rouge">./Wieghts/logs</code> directory for loading the pre-trained model.
     &gt;
     &gt; The generated segmentations will be saved in the output directory as separate image files.</p>
    </blockquote>
  </li>
</ol>


          </article>
          <h2>References</h2>
          <div class="publications">
            <h2 class="bibliography">2023</h2>
<ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/AttentionFilter.gif-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/AttentionFilter.gif-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/AttentionFilter.gif-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/AttentionFilter.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="AttentionFilter.gif" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div>

        <!-- Entry bib key -->
        <div id="Attention" class="col-sm-8">
        <!-- Title -->
        <div class="title">Attention Filter Gate U-Net: Learning from
Frequency domain for Medical image
Segmentation</div>
        <!-- Author -->
        <div class="author">
        

        Ahmad Wajeeh Yousef Elayan. Younes Elbrag</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em></em> 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a href="/assets/pdf/Thesis_Mastter.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          
        </div>
      </div>
</li></ol>
          </div>

        </div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Youness El Brag. Last updated: August 01, 2023.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

  </body>
</html>
